# -*- coding: utf-8 -*-
"""02_nn_classification_pytorch-video.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wuk4-IruMybXQJ4p8MY9NUimZLs_FQmd

# 02. Neural Network Classification With PyTorch

Classification: 1. Multiple 2. Binary

## Make Classfication Data and get ready
"""

import sklearn
from sklearn.datasets import make_circles

n_samples = 1000

X, y = make_circles(n_samples, noise=0.03, random_state=42)

len(X), len(y)

import pandas as pd
circles = pd.DataFrame({"X1": X[:, 0], "X2": X[:, 1], "label": y})
circles.head()

# visualise

import matplotlib.pyplot as plt

plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)

"""Note: Data we working with is often referreed to a toy data set"""

##Check Input and output Shapes
X.shape, y.shape

X_sample = X[0]
y_sampled = y[0]

import torch
torch.__version__

X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

#split data into training and test sets

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

## 2. Building a model

from torch import nn

device = 'cuda' if torch.cuda.is_available() else "cpu"
device

"""# Now we set up device agnositic code

Subclasses `nn.module`
create 2 `nn.linear()`
create `forward()` method
Instantiate an instance of our model class and sent it to the target device
"""

X_train, y_train[:5]

## Construct a model that sub classses nn.mmodule

class CircleModelV0(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features=2, out_features=5)
    self.layer_2 = nn.Linear(in_features=5, out_features=1)
    #self.two_linear_layers = nn.Sequential(
     #   nn.Linear(in_features=2, out_features=5),
     #   nn.Linear(in_features=5, out_features=1)
    #)

  def forward(self, x: torch.Tensor) -> torch.Tensor:
   return self.layer_2(self.layer_1(x)) #x-> layer(1) -> layer_2
   #return self.two_linear_layers(x)

model_0 = CircleModelV0().to(device)
model_0

next(model_0.parameters()).device

# lets replicate using nn.sequential

model_0 = nn.Sequential(
    nn.Linear(in_features=2, out_features=5),
    nn.Linear(in_features=5, out_features=1)
).to(device)

model_0

# make some predicitons with the mdoel
model_0.state_dict()

with torch.inference_mode():
  untrained_preds = model_0(X_test.to(device))
print(f"Length of preds: {len(untrained_preds)}")
print(f"Shape of preds: {untrained_preds.shape}")

y_test

## 2.1 set up loss function or optimiser

# binary cross entropy loss
# # for regression and classification we like BCE loss
# optimiser, Adam or SGD

# set up loss function
loss_fn = nn.BCELoss() # requires inputs to have gone through sigmoid activation function
loss_fn = nn.BCEWithLogitsLoss() #combines sigmpid layer with BCE loss
# this version is more numerically stable that sigmoid + BCE loss

optimiser = torch.optim.SGD(params=model_0.parameters(),lr=0.01)

# calculate accuracy out of 100 what % does our model get right
def accuracy_fn(y_true, y_pred):
  correct = torch.eq(y_true, y_pred).sum().item()
  acc = (correct/len(y_pred)) * 100
  return acc

"""#3. Train Model
1. Forward Pass
2.Calculate the loss
3. Optimise
4. Loss backward
Optimiser ste

"""

### Going from raw logits -> prdiction probablities -> labels

## model outputs are raw logits
## softmax for binary classification
## model predicitons probabilities to labels
## bby rounding or taking argmax(for outputs of softmax activation)
with torch.inference_mode():
  y_logits = model_0(X_test.to(device))
device

y_pred_probabs = torch.sigmoid(y_logits)
y_pred_probabs

y_preds = torch.round(y_pred_probabs)
y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))

torch.manual_seed(42)
torch.cuda.manual_seed(42)
epochs = 100

for epochs in range(epochs):
## training
    model_0.train()
    # forward pass
    y_logits = model_0(X_train.to(device)).squeeze()
    y_preds = torch.round(torch.sigmoid(y_logits)) # turn logits in to pred probs into pred labels
    # loss/accuracy
    # loss = loss_fn(torch.sigmoid(y_logits)) ## for BCE loss
    loss = loss_fn(y_logits.to(device), y_train.to(device)) ## nn.BCEwith Logits Loss expects raw logits as input
    acc = accuracy_fn(y_true=y_train.to(device), y_pred=y_preds.to(device))

    # Optimiser
    optimiser.zero_grad()
    loss.backward() # backprop
    optimiser.step() # gradient descent

    model_0.eval()
    with torch.inference_mode():
        test_logits = model_0(X_test.to(device)).squeeze()
        test_preds = torch.round(torch.sigmoid(test_logits))

        test_loss = loss_fn(test_logits.to(device), y_test.to(device))
        test_acc = accuracy_fn(y_true=y_test.to(device), y_pred=test_preds.to(device))

    #print whats happening
    if epochs % 10 == 0:
        print(f"Epoch: {epochs} | Loss: {loss:.5f} | Acc: {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%")

!wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py

from helper_functions import plot_predictions

from helper_functions import plot_decision_boundary

"""import requests
from pathlib import Path

# download helper funvtions from learn pytorch repo (if not downloaded)
if Path("helper_function.py").is_file():
    print("exits")
else:
    print("download helper function.py")
    request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/helper_functions.py")
    with open("helper_functions.py", "wb") as f:
        f.write(request.content)

from helper_functions import plot_predicitons, plot_decision_boundary"""



# plot decision boundary of the model
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_0, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_0, X_test, y_test)

"""## 5. Improving our model (from a models perspective)
* add more layers (give more chances to learn)
* add more hidden units - go from 5-10 hidden units
* fit for more epochs
* changing the activation function
* change the learning rate
* changing the loss function
"""

class CircleModelV1(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer_1 = nn.Linear(in_features=2, out_features=10)
        self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer
        self.layer_3 = nn.Linear(in_features=10, out_features=1)

    def forward(self, x): # note: always make sure forward is spelt correctly!
        # Creating a model like this is the same as below, though below
        # generally benefits from speedups where possible.
        # z = self.layer_1(x)
        # z = self.layer_2(z)
        # z = self.layer_3(z)
        # return z
        return self.layer_3(self.layer_2(self.layer_1(x)))

model_1 = CircleModelV1().to(device)
model_1

# loss_fn = nn.BCELoss() # Requires sigmoid on input
loss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input
optimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 1000 # Train for longer

# Put data to target device
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

for epoch in range(epochs):
    ### Training
    # 1. Forward pass
    y_logits = model_1(X_train).squeeze()
    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels

    # 2. Calculate loss/accuracy
    loss = loss_fn(y_logits, y_train)
    acc = accuracy_fn(y_true=y_train,
                      y_pred=y_pred)

    # 3. Optimizer zero grad
    optimizer.zero_grad()

    # 4. Loss backwards
    loss.backward()

    # 5. Optimizer step
    optimizer.step()

    ### Testing
    model_1.eval()
    with torch.inference_mode():
        # 1. Forward pass
        test_logits = model_1(X_test).squeeze()
        test_pred = torch.round(torch.sigmoid(test_logits))
        # 2. Caculate loss/accuracy
        test_loss = loss_fn(test_logits,
                            y_test)
        test_acc = accuracy_fn(y_true=y_test,
                               y_pred=test_pred)

    # Print out what's happening every 10 epochs
    if epoch % 100 == 0:
        print(f"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%")

# plot decision boundary of the model
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_1, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_1, X_test, y_test)

"""## 5.1 prepare date to see if model can fit a straight line.

One way to troubleshoot a larger problem is to test out a smaller problem
"""

# create some data

weight = 0.7
bias = 0.3
start = 0
end = 1
step = 0.01

#create data
X_regression = torch.arange(start, end, step).unsqueeze(dim=1)
y_regression = weight * X_regression + bias

print(len(X_regression), len(y_regression))

train_split = int(0.8 * len(X_regression))
X_train_reg, y_train_reg = X_regression[:train_split], y_regression[:train_split]
X_test_reg, y_test_reg = X_regression[train_split:], y_regression[train_split:]

len(X_train_reg), len(y_train_reg), len(X_test_reg), len(y_test_reg)

plot_predictions(train_data=X_train_reg,
                 train_labels=y_train_reg,
                 test_data=X_test_reg,
                 test_labels=y_test_reg,
                 predictions=None)

# adjust model 1 to fit a straight line data

# same architecture as model1, using nn.sequential

model_2 = nn.Sequential(
    nn.Linear(in_features=1, out_features=10),
    nn.Linear(in_features=10, out_features=10),
    nn.Linear(in_features=10, out_features=1)
).to(device)

loss_fn = nn.L1Loss()
optimizer = torch.optim.SGD(model_2.parameters(), lr=0.01)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 1000

X_train_reg, y_train_reg = X_train_reg.to(device), y_train_reg.to(device)
X_test_reg, y_test_reg = X_test_reg.to(device), y_test_reg.to(device)

for epochs in range(epochs):
  y_pred = model_2(X_train_reg)
  loss = loss_fn(y_pred, y_train_reg)

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  model_2.eval()
  with torch.inference_mode():
    test_pred = model_2(X_test_reg)
    test_loss = loss_fn(test_pred, y_test_reg)

#print out
if epochs % 100 == 0:
  print(f"Epoch: {epochs} | Loss: {loss:.5f} | Test Loss: {test_loss:.5f}")

# turn  on evaluation
model_2.eval()
with torch.inference_mode():
  y_preds = model_2(X_test_reg)
plot_predictions(train_data=X_train_reg.cpu(),
                 train_labels=y_train_reg.cpu(),
                 test_data=X_test_reg.cpu(),
                 test_labels=y_test_reg.cpu(),
                 predictions=y_preds.cpu())

"""##  6 the missing piece of our model: non linearity
"what patterns to draw if given an inifnite amount of straight or non-straight line?"

"""

### 6.1 recreating non-linearity

# make and plot data
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles

n_samples = 1000

X, y = make_circles(n_samples, noise=0.03, random_state=42)

plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)

# convert the data to tensors then to train and test

import torch
from sklearn.model_selection import train_test_split

X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

# split into train the test split

X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train[:5], y_train[:5]

### 6.2 Building a model with non-linearity

from torch import nn

class CircleModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features=2, out_features=10)
    self.layer_2 = nn.Linear(in_features=10, out_features=10)
    self.layer_3 = nn.Linear(in_features=10, out_features=1)
    self.relu = nn.ReLU()

  def forward(self, x):
    return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))

model_3 = CircleModelV2().to(device)
model_3

# setu[p a loss and ooptimiser]

loss_fn = nn.BCEWithLogitsLoss()
optimiser = torch.optim.SGD(model_3.parameters(), lr=0.1)

# Training our model

torch.manual_seed(42)
torch.cuda.manual_seed(42)

X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

epochs = 1000

for epochs in range(epochs):
  model_3.train()
  y_logits = model_3(X_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))

  loss = loss_fn(y_logits, y_train)
  acc = accuracy_fn(y_true=y_train, y_pred=y_pred)
  optimiser.zero_grad()
  loss.backward()
  optimiser.step()

  model_3.eval()
  with torch.inference_mode():
    test_logits = model_3(X_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))
    test_loss = loss_fn(test_logits, y_test)
    test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)

  if epochs % 100 == 0:
    print(f"Epoch: {epochs} | Loss: {loss:.5f} | Acc: {acc:.2f}% | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%")

model_3.eval()
with torch.inference_mode():
  y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()
y_preds[:10]

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_3, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_3, X_test, y_test)

"""## 7. Replicating non linear activation function

"""

A = torch.arange(-10,10,1,dtype = torch.float32)
A.dtype

def relu(x):
  return torch.maximum(torch.tensor(0), x)

relu(A)

#plot Relu Activation Function
plt.plot(relu(A))

def sigmoid(x):
  return 1/(1+torch.exp(-x))

plt.plot(sigmoid(A))

# Import dependencies
import torch
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split

# Set the hyperparameters for data creation
NUM_CLASSES = 5
NUM_FEATURES = 2
RANDOM_SEED = 42

# 1. Create multi-class data
X_blob, y_blob = make_blobs(n_samples=1000,
    n_features=NUM_FEATURES, # X features
    centers=NUM_CLASSES, # y labels
    cluster_std=1.6, # give the clusters a little shake up (try changing this to 1.0, the default)
    random_state=RANDOM_SEED
)

# 2. Turn data into tensors
X_blob = torch.from_numpy(X_blob).type(torch.float)
y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)
print(X_blob[:5], y_blob[:5])

# 3. Split into train and test sets
X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,
    y_blob,
    test_size=0.2,
    random_state=RANDOM_SEED
)

# 4. Plot data
plt.figure(figsize=(10, 7))
plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);

"""## Building a multiclass classification model in pytorch"""

# Create device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

class BlobModel(nn.Module):
  def __init__(self, input_features, output_features, hidden_units=8):
    """Initialises multiclass classification model
    Args:
    input_features (int)
    output_features (int)
    hidden_units (int, optional)


    """
    super().__init__()
    self.linear_layer_stack = nn.Sequential(
        nn.Linear(in_features=input_features, out_features=hidden_units),
        nn.ReLU(),
        nn.Linear(in_features=hidden_units, out_features=hidden_units),
        nn.ReLU(),
        nn.Linear(in_features=hidden_units, out_features=output_features)
    )

  def forward(self, x):
    return self.linear_layer_stack(x)

model_4 = BlobModel(input_features=2, output_features=5, hidden_units=10).to(device)

model_4

# create a loss function multiclass classification
loss_fn = nn.CrossEntropyLoss()
optimiser = torch.optim.SGD(model_4.parameters(), lr=0.1)

# building the traiing loops
model_4.eval()
with torch.inference_mode():
  y_logits = model_4(X_blob_test.to(device))

y_logits[:10]

y_blob_test[:10]

# convert model's oogits output to prediction prob

y_pred_prob = torch.softmax(y_logits, dim=1)
print(y_pred_prob[:5])
print(y_logits[:5])

torch.max(y_pred_prob[0])

# convert our model's prediciton probabblities to prediction labels

y_preds = torch.argmax(y_pred_prob, dim=1)
y_preds[:10]

y_blob_test[:10]

## Building our first training and testing loop

torch.manual_seed(42)
torch.cuda.manual_seed(42)
# fit the multiclass model

epochs = 1000

X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)
X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)

for epochs in range(epochs):
  model_4.train()
  y_logits = model_4(X_blob_train).squeeze()
  y_pred = torch.softmax(y_logits, dim=1)
  y_pred_labels = torch.argmax(y_pred, dim=1)

  loss = loss_fn(y_logits, y_blob_train)
  acc  = accuracy_fn(y_true=y_blob_train,y_pred=y_pred_labels)
  optimiser.zero_grad()
  loss.backward()
  optimiser.step()

  model_4.eval()
  with torch.inference_mode():
    test_logits = model_4(X_blob_test).squeeze()
    test_pred = torch.softmax(test_logits, dim=1)
    test_pred_labels = torch.argmax(test_pred, dim=1)
    test_loss = loss_fn(test_logits, y_blob_test)
    test_acc = accuracy_fn(y_true=y_blob_test, y_pred=test_pred_labels)

  if epochs % 100 == 0:
   print(f"Epoch: {epochs} | Loss: {loss:.5f} | Acc: {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%")

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_4, X_blob_train, y_blob_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_4, X_blob_test, y_blob_test)

"""## A few more classification matrix (to evaluate our model)
* accuracy
* F1 score
* precision
* confusion matrix
* recall
* classification report


"""

!pip install torchmetrics

from torchmetrics import Accuracy

torch_metrics_acc = Accuracy(task="multiclass", num_classes=5).to(device)
torch_metrics_acc(y_blob_test, test_pred_labels)

