# -*- coding: utf-8 -*-
"""00_pytorch1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12LvR3t06aJmRkOMRXKA7HojSU55Ge3tt
"""

##00

"""print("Hello")"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
print(torch.__version__)

## intro to tensors

### creating tensors

#scalar
scalar = torch.tensor(7)
scalar

scalar.ndim

## get tensor back as python list
scalar.item()

vector = torch.tensor([7,7])
vector

vector.ndim

vector.shape

matrix = torch.tensor([[7,8],
                      [9,10]])
matrix

"""matrix.shape"""

matrix[:,0]

matrix[0,0]

"""##TENSOR"""

TENSOR = torch.tensor([[[1,2,3],
                       [3,6,9],
                       [2,4,5]]])
TENSOR

TENSOR.shape

"""`dim = 0 [1,2,3][3,6,9][2,4,5]
dim = 1 [],[],[]
dim = 2 [.,.,.]`
"""

TENSOR[0]

TENSOR[0,0,2]

##random tensors

## why random tensors?

rando = torch.rand(3,4)
rando

rando[1,2]

rando.ndim

random_image_size_tensor = torch.rand(size=(3, 224,224))
random_image_size_tensor.shape, random_image_size_tensor.ndim

## Zeroes and Ones
zeros = torch.zeros(size=(3,4))
zeros

zeros*rando

ones = torch.ones(size=(3,4))
ones

ones.dtype #default data type

### creat a range odf tensors and tensors like

one_to_ten = torch.arange(start=1, end=11, step=1)
one_to_ten

## creating tensors like
ten_zeroes_like = torch.zeros_like(input=one_to_ten)
ten_zeroes_like

"""##17 Tensor Datatype"""

float_32_tensor = torch.tensor([3.0,6.0,9.0],
                               dtype=None,
                               device='cpu', #which memeory is the tensor on?
                               requires_grad=False)# to track gradients
float_32_tensor

float_32_tensor.dtype

float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor

float_16_tensor*float_32_tensor

int_32_tensor = torch.tensor([3,6,9], dtype=torch.int32)
int_32_tensor

## getting inf from tensors

##tensor.dtype
##tensor.shape
##tensor.device

some_tensor = torch.rand(3,4,dtype=torch.float16)
some_tensor

some_tensor.size(), some_tensor.shape

some_tensor.device, some_tensor.dtype

tensor = torch.tensor([1,2,3])
tensor+10

tensor = tensor*10
tensor

tensor - 10

## pytorch inbuilt
torch.mul(tensor,10)

torch.add(tensor, 20)

##multiplication matrix
## element wise multiplication
## matrx multiplication
tensor = torch.tensor([[1,1,1],[1,1,1]])
tensor2 = torch.tensor([[2,3],[3,4],[4,5]])
torch.matmul(tensor2,tensor)

# shapes for matrix mult

tensor_a = torch.tensor([[1,2],[3,4],[5,6]])

tensor_b = torch.tensor([[7,10],[8,11],[9,12]])

torch.mm(tensor_b.T,tensor_a)

#finding the min, max, mean of the tensor

x = torch.arange(1,100,10)
torch.mean(x.type(torch.float32)), x.min(), x.max()

x.type(torch.float32).mean()

torch.sum(x)

x.argmax() ## position that has max value
x.argmin() ## position with min value

x[9]

## reshaping, stacking, squeezing and unsqueezing and permute



x = torch.arange(1.,10.)

x_reshaped = x.reshape(1,9)
x_reshaped, x_reshaped.shape

z = x.view(1,9)
z, z.shape

#changing z changes x

z[:,0] = 5
z, x

# stack tensors
x_stacked = torch.stack([x,x,x,x], dim=1)
x_stacked, x_stacked.shape

## squeeze and unsqueeze

# squeeze = removes all single dimensions

x_reshaped.shape

x_squeeze = x_reshaped.squeeze()

x_unsqueeze = x_squeeze.unsqueeze(dim = 0)

x_unsqueeze.shape

# permute = used moslty with images
x_orginal = torch.rand(size=(224,224,3))

# permute the original tensor to rearrange the axis (or dim) order
x_permuted = x_orginal.permute(2,0,1) # shifts axis 0 to 1 , 1 > 2 and 2 to 0
x_orginal.shape, x_permuted.shape

## indexing

#indexing iwth numpy and pytorch is same

x = torch.arange(1,10).reshape(1,3,3)
x

#lets index on our new tensor
x[0]

x[:,1,1]

x

#numpy is a popular computing library

#torch.from_numpy(ndarray)
#torch.Tensor.numpy()

import numpy as np
array = np.arange(1.0, 8.0)
tensor = torch.from_numpy(array)
array, tensor

#but why float 64

#change values of array

array = array + 1
tensor, array

tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor

##reproductibilty (trying to make random out of random)
torch.rand(3,3)

## to reduce the randomess in a neural networks and putorch comes with
## the concept of a random seed

random_tesnor_a = torch.rand(3,4)
random_tesnor_b = torch.rand(3,4)

print(random_tesnor_a)
print(random_tesnor_b)
print(random_tesnor_a == random_tesnor_b)

# lets make some random reproducatble tensors

#set random seed
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
random_tesnor_c = torch.rand(3,4)
torch.manual_seed(RANDOM_SEED)
random_tesnor_d = torch.rand(3,4)

print(random_tesnor_c)
print(random_tesnor_d)
print(random_tesnor_c == random_tesnor_d)

## running tensors and pytorch objects on GPU
"""
  getting a gpu
  1. google collab gpu
  2. use your own gpu
  3. use cloud computing - GCP, AWS, Azure
"""

!nvidia-smi

torch.cuda.is_available()

## setup device agnostic code

device = 'cuda' if torch.cuda.is_available() else "cpu"
device

torch.cuda.device_count()

#python set agnostic codee

# putting the code on gpu makes it faster

# cant covert tensor on gpu to numpy

"""import torch
print(torch.cuda.is_available())
device = "cuda" if torch.cuda.is_available() else "cpu"
##gpu great at numerical calc

tensor = torch.tensor([1,2,3])
## move tensor to gpu if available

tensor_gpu = tensor.to(device)
print(tensor_gpu, tensor_gpu.device)

tensor_back_on_cpu = tensor_gpu.cpu().numpy()
print(tensor_back_on_cpu)"""

